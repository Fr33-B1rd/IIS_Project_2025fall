import torch
import cv2
import numpy as np
from torchvision import transforms
from model_def import SimpleEmotionMLP
import time
import torch.nn.functional as F

class EmotionDetector:
    def __init__(self, model_path="models/emotion_classifier.pth"):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        # 1. Load DINOv2
        print(f"Loading Perception on {self.device}...")
        self.backbone = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14').to(self.device)
        self.backbone.eval()
        
        # 2. Load Classifier
        self.classifier = SimpleEmotionMLP().to(self.device)
        try:
            self.classifier.load_state_dict(torch.load(model_path, map_location=self.device))
            self.classifier.eval()
            print("Model loaded successfully.")
        except Exception as e:
            print(f"WARNING: Model load failed ({e}). Running in dummy mode.")

        # 3. Preprocessing
        self.transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        
        # LABELS: 0=Excited, 1=Nervous, 2=Confused
        self.labels = ["Excited", "Nervous", "Confused"]
        
        # 4. Camera Setup
        self.cap = cv2.VideoCapture(0) # Change to your IP Camera URL if needed
        self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
        self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
        self.cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)

        # Performance Vars
        self.frame_count = 0
        self.skip_rate = 4
        self.last_emotion = "Neutral"
        
        # --- NEW: CALIBRATION VARS ---
        self.baseline_logits = torch.zeros(1, 3).to(self.device)
        self.is_calibrated = False

    def calibrate(self):
        """Captures the user's 'Resting Face' to remove model bias."""
        print("\n[CALIBRATION] Look at the camera with a NEUTRAL/RESTING face...")
        print("Capturing in 3 seconds...")
        time.sleep(1)
        print("2...")
        time.sleep(1)
        print("1...")
        time.sleep(1)
        
        logits_sum = torch.zeros(1, 3).to(self.device)
        samples = 0
        
        # Take 10 frames to average
        for _ in range(10):
            ret, frame = self.cap.read()
            if not ret: continue
            
            # Process Frame
            img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            img_t = torch.from_numpy(img_rgb).permute(2, 0, 1).float() / 255.0
            img_t = self.transform(img_t).unsqueeze(0).to(self.device)
            
            with torch.no_grad():
                feat = self.backbone(img_t)
                out = self.classifier(feat)
                logits_sum += out
                samples += 1
            time.sleep(0.1)
            
        if samples > 0:
            self.baseline_logits = logits_sum / samples
            self.is_calibrated = True
            print(f"[CALIBRATION] Baseline set: {self.baseline_logits.cpu().numpy()}")
            print("System is now tuned to YOUR face.\n")
        else:
            print("[CALIBRATION] Failed to capture frames.")

    def get_emotion(self):
        ret, frame = self.cap.read()
        if not ret: return "Neutral"
        
        self.frame_count += 1

        # Frame Skipping
        if self.frame_count % self.skip_rate != 0:
            cv2.imshow("DM Vision", frame)
            cv2.waitKey(1)
            return self.last_emotion

        # Inference
        img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        img_t = torch.from_numpy(img_rgb).permute(2, 0, 1).float() / 255.0
        img_t = self.transform(img_t).unsqueeze(0).to(self.device)
        
        with torch.no_grad():
            features = self.backbone(img_t)
            raw_logits = self.classifier(features)
            
            # --- NEW: APPLY CALIBRATION ---
            # We subtract the "resting face" scores from the current scores.
            # This highlights *changes* in expression rather than absolute values.
            if self.is_calibrated:
                adjusted_logits = raw_logits - self.baseline_logits
            else:
                adjusted_logits = raw_logits

            # Get Probabilities (for debugging)
            probs = F.softmax(adjusted_logits, dim=1)
            _, predicted = torch.max(adjusted_logits, 1)
            
        current_emotion = self.labels[predicted.item()]
        self.last_emotion = current_emotion
        
        # --- DEBUG OVERLAY ---
        # Shows bars for Excited, Nervous, Confused
        scores = probs.cpu().numpy()[0]
        debug_text = f"E:{scores[0]:.2f} N:{scores[1]:.2f} C:{scores[2]:.2f}"
        
        # Color: Red if Nervous, Green if Excited, Blue if Confused
        color = (0, 255, 0)
        if current_emotion == "Nervous": color = (0, 0, 255)
        elif current_emotion == "Confused": color = (255, 0, 0)

        cv2.putText(frame, f"Detected: {current_emotion}", (10, 30), 
                    cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)
        cv2.putText(frame, debug_text, (10, 60), 
                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (200, 200, 200), 1)
        
        cv2.imshow("DM Vision", frame)
        cv2.waitKey(1)
        
        return current_emotion